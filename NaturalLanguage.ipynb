{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I am planning to buy a new branded sports car and doing lot of research on it. Possiblly I will buy next year. Do you know any sports cars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentense tokenizer\n",
      "['I am planning to buy a new branded sports car and doing lot of research on it.', 'Possiblly I will buy next year.', 'Do you know any sports cars']\n"
     ]
    }
   ],
   "source": [
    "print (\"Sentense tokenizer\")\n",
    "print (sent_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizer\n",
      "['I', 'am', 'planning', 'to', 'buy', 'a', 'new', 'branded', 'sports', 'car', 'and', 'doing', 'lot', 'of', 'research', 'on', 'it', '.', 'Possiblly', 'I', 'will', 'buy', 'next', 'year', '.', 'Do', 'you', 'know', 'any', 'sports', 'cars']\n"
     ]
    }
   ],
   "source": [
    "print (\"Word tokenizer\")\n",
    "print (word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word punct tokenizer\n",
      "['I', 'am', 'planning', 'to', 'buy', 'a', 'new', 'branded', 'sports', 'car', 'and', 'doing', 'lot', 'of', 'research', 'on', 'it', '.', 'Possiblly', 'I', 'will', 'buy', 'next', 'year', '.', 'Do', 'you', 'know', 'any', 'sports', 'cars']\n"
     ]
    }
   ],
   "source": [
    "print (\"Word punct tokenizer\")\n",
    "words = WordPunctTokenizer().tokenize(input_text)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting words to their Base forms\n",
    "Stemming Algorithems</br>\n",
    "Porter    - Least strict (slow)</br>\n",
    "Lancaster - Strictest (Fast)</br>\n",
    "Snowball  - Moderate (Fast)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT Word          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "\n",
      "                I               I               I               i \n",
      " ====================================================================\n",
      "\n",
      "               am              am              am              am \n",
      " ====================================================================\n",
      "\n",
      "         planning        planning            plan            plan \n",
      " ====================================================================\n",
      "\n",
      "               to              to              to              to \n",
      " ====================================================================\n",
      "\n",
      "              buy             buy             buy             buy \n",
      " ====================================================================\n",
      "\n",
      "                a               a               a               a \n",
      " ====================================================================\n",
      "\n",
      "              new             new             new             new \n",
      " ====================================================================\n",
      "\n",
      "          branded         branded           brand           brand \n",
      " ====================================================================\n",
      "\n",
      "           sports          sports           sport           sport \n",
      " ====================================================================\n",
      "\n",
      "              car             car             car             car \n",
      " ====================================================================\n",
      "\n",
      "              and             and             and             and \n",
      " ====================================================================\n",
      "\n",
      "            doing           doing              do           doing \n",
      " ====================================================================\n",
      "\n",
      "              lot             lot             lot             lot \n",
      " ====================================================================\n",
      "\n",
      "               of              of              of              of \n",
      " ====================================================================\n",
      "\n",
      "         research        research        research        research \n",
      " ====================================================================\n",
      "\n",
      "               on              on              on              on \n",
      " ====================================================================\n",
      "\n",
      "               it              it              it              it \n",
      " ====================================================================\n",
      "\n",
      "                .               .               .               . \n",
      " ====================================================================\n",
      "\n",
      "        Possiblly       Possiblly       possiblli            poss \n",
      " ====================================================================\n",
      "\n",
      "                I               I               I               i \n",
      " ====================================================================\n",
      "\n",
      "             will            will            will             wil \n",
      " ====================================================================\n",
      "\n",
      "              buy             buy             buy             buy \n",
      " ====================================================================\n",
      "\n",
      "             next            next            next            next \n",
      " ====================================================================\n",
      "\n",
      "             year            year            year            year \n",
      " ====================================================================\n",
      "\n",
      "                .               .               .               . \n",
      " ====================================================================\n",
      "\n",
      "               Do              Do              Do              do \n",
      " ====================================================================\n",
      "\n",
      "              you             you             you             you \n",
      " ====================================================================\n",
      "\n",
      "             know            know            know            know \n",
      " ====================================================================\n",
      "\n",
      "              any             any             ani             any \n",
      " ====================================================================\n",
      "\n",
      "           sports          sports           sport           sport \n",
      " ====================================================================\n",
      "\n",
      "             cars            cars             car             car \n",
      " ====================================================================\n"
     ]
    }
   ],
   "source": [
    "stemmer_names = [\"PORTER\", \"LANCASTER\", \"SNOWBALL\"]\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print ('\\n', formatted_text.format('INPUT Word', *stemmer_names), '\\n','='*68)\n",
    "for word in words:\n",
    "    output = [word, porter.stem(word), lancaster.stem(word), snowball.stem(word)]\n",
    "    print ('\\n', formatted_text.format(word, *output), '\\n','='*68)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " INPUT WORD              NOUN LEMMATIZER         VERB LEMMATIZER          \n",
      " ===========================================================================\n",
      "I                       I                       I                       \n",
      "am                      am                      be                      \n",
      "planning                planning                plan                    \n",
      "to                      to                      to                      \n",
      "buy                     buy                     buy                     \n",
      "a                       a                       a                       \n",
      "new                     new                     new                     \n",
      "branded                 branded                 brand                   \n",
      "sports                  sport                   sport                   \n",
      "car                     car                     car                     \n",
      "and                     and                     and                     \n",
      "doing                   doing                   do                      \n",
      "lot                     lot                     lot                     \n",
      "of                      of                      of                      \n",
      "research                research                research                \n",
      "on                      on                      on                      \n",
      "it                      it                      it                      \n",
      ".                       .                       .                       \n",
      "Possiblly               Possiblly               Possiblly               \n",
      "I                       I                       I                       \n",
      "will                    will                    will                    \n",
      "buy                     buy                     buy                     \n",
      "next                    next                    next                    \n",
      "year                    year                    year                    \n",
      ".                       .                       .                       \n",
      "Do                      Do                      Do                      \n",
      "you                     you                     you                     \n",
      "know                    know                    know                    \n",
      "any                     any                     any                     \n",
      "sports                  sport                   sport                   \n",
      "cars                    car                     cars                    \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_names = [\"NOUN LEMMATIZER\", \"VERB LEMMATIZER\"]\n",
    "format_text = '{:24}' * (len(lemmatizer_names) + 1)\n",
    "print ('\\n', format_text.format('INPUT WORD', *lemmatizer_names),'\\n','='*75)\n",
    "for word in words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
    "    print (format_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing text data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "    cur_chunk = []\n",
    "    count = 0 \n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "    output.append(' '.join(cur_chunk))\n",
    "    return output\n",
    "input_data = ' '.join(brown.words()[:12000])\n",
    "chunk_size = 700\n",
    "chunks = chunker(input_data, chunk_size)\n",
    "print (\"\\nNumber of text chunks =\",len(chunks), \"\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print ('Chunk', i+1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words model - document-term metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'text_chunker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-83f9a29c904f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_chunker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'text_chunker'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import brown\n",
    "from text_chunker import chunker\n",
    "\n",
    "input_data =' '.join(brown.words()[:5400])\n",
    "chunk_size = 800\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "chunks = []\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)\n",
    "count_vectorizer = CountVectorizer(min_df=7, max_df=20)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print (\"\\nVocabulary:\\n\", vocabulary)\n",
    "chunk_names = []\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append('Chunk-'+str(i))\n",
    "    \n",
    "print (\"\\nDocument Term matrix:\")\n",
    "formatted_text = '{:>12}'*(len(chunk_names)+1)\n",
    "print (\"\\n\", formatted_text.format('Word', *chunk_names), '\\n')\n",
    "for word,item in zip(vocabulary, document_term_matrix.T):\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print (formatted_text.format(*output))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category predictor - bag of words model\n",
    "### tf-idf (TermFrequency-InverseDcoumentFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training data : (2844, 40321)\n",
      "\n",
      "Input : You need to be careful with cars when you are driving on slippery roads \n",
      "Predicted category : Autos\n",
      "\n",
      "Input : A lot of devices can be operated wirelessly \n",
      "Predicted category : Electronics\n",
      "\n",
      "Input : Players need to be careful when they are close to goal posts \n",
      "Predicted category : Hockey\n",
      "\n",
      "Input : Political debates help us undestand the prespectives of both sides \n",
      "Predicted category : Politics\n",
      "\n",
      "Input : When you are crossing junctions, be careful \n",
      "Predicted category : Autos\n",
      "\n",
      "Input : my chairs are very old and I want to change them \n",
      "Predicted category : Autos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos',\n",
    "               'rec.sport.hockey':'Hockey', 'sci.electronics': 'Electronics',\n",
    "               'sci.med': 'Medicine'}\n",
    "\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=5)\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
    "print(\"\\nDimensions of training data :\", train_tc.shape)\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)\n",
    "\n",
    "# Define data sets\n",
    "input_data = [\n",
    "    \"You need to be careful with cars when you are driving on slippery roads\",\n",
    "    \"A lot of devices can be operated wirelessly\",\n",
    "    \"Players need to be careful when they are close to goal posts\",\n",
    "    \"Political debates help us undestand the prespectives of both sides\",\n",
    "    \"When you are crossing junctions, be careful\",\n",
    "    \"my chairs are very old and I want to change them\"\n",
    "]\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
    "input_tc = count_vectorizer.transform(input_data)\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "predictions = classifier.predict(input_tfidf)\n",
    "for sent, category in zip(input_data,predictions):\n",
    "    print (\"\\nInput :\", sent, '\\nPredicted category :', category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a Gender Identifier\n",
    "#### Using heuristic to construct feature vector\n",
    "#### Using it to train classifier\n",
    "##### Name ends with ia - female name ( Amelia or Genelia)\n",
    "##### Name ends with rk - make name (Mark or Clark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(word, N=2):\n",
    "    last_n_letters = word[-N:]\n",
    "    return {'feature': last_n_letters.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_list = [(name, 'male') for name in names.words('male.txt')]\n",
    "female_list = [(name, 'female') for name in names.words('female.txt')]\n",
    "data = (male_list + female_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of end letters : 1\n",
      "Accuracy =75.9%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> male\n",
      "prabhakar ==> male\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 2\n",
      "Accuracy =77.53%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> male\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 3\n",
      "Accuracy =76.53%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 4\n",
      "Accuracy =70.17%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 5\n",
      "Accuracy =64.44%\n",
      "Alexander ==> male\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 6\n",
      "Accuracy =61.23%\n",
      "Alexander ==> female\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 7\n",
      "Accuracy =59.97%\n",
      "Alexander ==> female\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n",
      "\n",
      "Number of end letters : 8\n",
      "Accuracy =59.66%\n",
      "Alexander ==> female\n",
      "Danielle ==> female\n",
      "David ==> male\n",
      "Cheryl ==> female\n",
      "prabhakar ==> female\n",
      "harsha ==> female\n"
     ]
    }
   ],
   "source": [
    "random.seed(5)\n",
    "random.shuffle(data)\n",
    "input_names = ['Alexander', 'Danielle', 'David','Cheryl', 'prabhakar', 'harsha']\n",
    "num_train = int(0.8*len(data))\n",
    "for i in range(1,9):\n",
    "    print (\"\\nNumber of end letters :\", i)\n",
    "    features = [(extract_features(n,i), gender) for (n,gender) in data]\n",
    "    train_data,test_data = features[:num_train], features[num_train:]\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    accuracy = round(100*nltk_accuracy(classifier, test_data), 2)\n",
    "    print (\"Accuracy =\"+str(accuracy)+'%')\n",
    "    for name in input_names:\n",
    "        print (name, \"==>\", classifier.classify(extract_features(name,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF model for predecting the correct words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
